**Hands-on Lab: Host your own LLM with the Red Hat vLLM Inference Server**

**Welcome!**

In this lab, you'll run two optimized versions of the same Mistral model on RHEL AI with GPUs and see how their quality and behavior compare.

We'll use:

* A standard Red Hat AI Inference Server (RHAIIS) container
* Two pre-optimized Mistral variants (already compressed/quantized by Red Hat)
* A small evaluation tool to compare them

No model training, no long-running compression jobs - everything you do should be visible within the 90 minutes.

You'll work in teams of 2-3 in a shared environment.
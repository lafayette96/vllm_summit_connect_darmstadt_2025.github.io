# Hands-on Lab: Compare Optimized Mistral Models on RHEL AI (90 min)

## Welcome!

In this lab, you'll run two optimized versions of the same Mistral model on RHEL AI with GPUs and see how their quality and behavior compare.

We'll use:

* A standard Red Hat AI Inference Server container
* Two pre-optimized Mistral variants (already compressed/quantized by Red Hat)
* A small evaluation tool to compare them

No model training, no long-running compression jobs - everything you do should be visible within the 90 minutes.

You'll work in teams of 2-3 in a shared environment.